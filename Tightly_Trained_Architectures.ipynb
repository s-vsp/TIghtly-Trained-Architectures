{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rr_eNvAZGD1"
      },
      "source": [
        "# Tightly Trained Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ9xLsgnZCBa"
      },
      "source": [
        "## 1. Wstęp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDAKALzvZYbX"
      },
      "source": [
        "W ramach ćwiczenia realizowana jest praca na pięciu architekturach głębokich sieci neuronowych w zadaniach klasyfikacji. W szczególności treść laboratorium poświęcona zostanie analizie modeli przy pracy na danych o charakterze \"in the wild\", a zatem surowych obrazach zaczerpniętych ze zdjęć różnego rodzaju bydynków z ulic amerykańskich miast oraz zdjęć mieszkań z ogłoszeń internetowych. Głównym celem zadań w tym laboratorium jest badanie w jaki sposób zachowają się wytrenowane sieci (trenowane w ściśle określonej konfiguracji warstw, wielkości oraz hiperaparamterów) na danych surowo pobranych z [Map Google](https://www.google.com/maps) oraz z [Airbnb](https://www.airbnb.pl/). Zbadane zostaną następujące architektury:\n",
        "1. [VGG-16](https://arxiv.org/pdf/1409.1556v6.pdf) - klasyczna już głęboka sieć konwolucyjna o składająca się z 16 warstw (13 konwolucyjnych i 3 fully-connected), której cechą charakterystyczną  jest zastosowanie wielu warstw konwolucyjnych o niewielkich filtrach (3x3), co przyczynia się do jej zdolności do efektywnego uczenia hierarchicznych cech obrazów.\n",
        "2. [ResNet50](https://arxiv.org/pdf/2110.00476.pdf) - Residual Network (sieć resztkowa), model głębokiego uczenia, w którym warstwy wagowe uczą się funkcji resztkowych w odniesieniu do danych wejściowych warstwy. Jest specyficznym typem konwolucyjnej sieci neuronowej. Składa sie z 50 warstw (48 konwolucyjnych, jedna MaxPool oraz jedna średniej puli). Posiada mniej filtrów niż VGG oraz jest mniej złożona. Architektura tej sieci opiera się na dwóch zasadach: Liczba filtrów jest taka sama w każdej warstwie w zależności od rozmiaru wyjściowej mapy cech. Jeśli rozmiar mapy cech jest zmniejszony o połowę, ma ona podwójną liczbę filtrów, by utrzymać złożoność czasową każdej warstwy.\n",
        "3. [SwinTransformer](https://arxiv.org/pdf/2103.14030v2.pdf) - nowy rodzaj transformera wizyjnego, zaprojektowany do ogólnego zastosowania w wizji komputerowej. Charakteryzuje go hierarchiczna architektura, pozwalająca na łączenie sąsiednich fragmentów obrazu w głebszych warstawch. Pozwala ona na skalowanie i wykorzystywania na różnych rozmiarach. W Swin Transformerze, samoorganizacja jest obliczana w obrębie lokalnych, nienakładających się okien. Te okna są przesuwane między kolejnymi warstwami transformatora, co umożliwia połączenia międzyokienne i zwiększa moc modelowania. Dzięki obliczaniu samoorganizacji w ograniczonych lokalnych oknach, osiąga on liniową złożoność obliczeniową względem rozmiaru obrazu, co czyni go skalowalnym. Dzięki tym cechom model osiąga wysoką wydajność w zadaniach tj. klasyfikacja obrazów, detekcja obiektów czy segmentacja.\n",
        "4. [ConvNeXt](https://openaccess.thecvf.com//content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf) - konwolucyjna odpowiedź na state-of-the-art sieci transformerowe, gdzie od strony architektonicznej sieć ta jest formą modernizacji sieci ResNet poprzez zastosowanie schematów projektowych sprawdzających się w transfomerach, a więc między innymi: \"stem\" cell zmieniony na \"patchify\" cell (nie nakładająca się konwolucja)​, większy kernel size (7x7), inverted bottlneck, depthiwise convolution oraz liczne zmiany micro-designu takie jak nieliniowość GeLU, Layer Normalization, czy Layer Scale.\n",
        "5. [ConvNeXt V2](https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf) - modyfikacja oryginalnego ConvNeXt'a a dwa dodatkowe rozwiązania projektowe, mianowicie zastąpienie Layer Scale przez Global Response Normalization (GRN) do promocji różnorodności cech w trakcie treningu, a także co ważniejsze wykorzystanie self-supervised learning'u z wykorzystaniem Masked Autoencoderów (MAE) do pretreningu sieci.\n",
        "\n",
        "Każda z wspomnianych sieci wytrenowana została z optymalneym zestawieniem hiperparametrów zasugerowanym w oryginalnych artykułach, a proces treningu realizowany był jako transfer learning w którym szkolona była głowa klasyfikacyjna każdej sieci.\n",
        "\n",
        "Link do checkpointów dla każdego z modeli: https://drive.google.com/drive/folders/1i-3OwNELUbMxv7CGIuQy_xygln_9OwCA\n",
        "\n",
        "Drugim etapem badań sieci była analiza ich interpretowalności w związku z czym wykorzystana została technika [Integrated Gradients](https://arxiv.org/pdf/1703.01365.pdf).\n",
        "Jednym z głównych wymagań dotyczących interpretowalności modeli jest aksjomat wrażliwości, który ma dwa główne aspekty:\n",
        "\n",
        "- Jeśli baseline i dane wejściowe różnią się tylko jedną cechą, ale mają różne przewidywania, wówczas ta cecha otrzymuje niezerowe przypisanie.\n",
        "- Jeśli cecha nie odgrywa żadnej roli w sieci, nie otrzymuje żadnych przypisań.\n",
        "\n",
        "Prosta metoda, taka jak mnożenie danych wejściowych przez ich gradient, nie spełnia tego wymogu, na przykład dla $F(x) = 1 - max(0, x)$ wymóg ten nie jest spełniony. Metoda taka jak Integrated Gradients działa dobrze dla tego wymógu. Z definicji są one obliczane w następujący sposób:\n",
        "\n",
        "$\\text{IntegratedGrads}_i(x) = (x - x') \\times \\int_{\\alpha=0}^1 \\frac{\\partial f(x'+\\alpha \\times (x-x'))}{\\partial x_i} \\, d\\alpha$\n",
        "\n",
        "Proces obliczania zintegrowanych gradientów jest następujący:\n",
        "\n",
        "1. Wybierz linię bazową lub punkt odniesienia dla funkcji wejściowych. Reprezentuje on punkt, w którym dane wyjściowe modelu są znane (zazwyczaj jest to punkt, w którym wszystkie cechy są ustawione na zero - czarny obraz).\n",
        "\n",
        "2. Utwórz ścieżkę od baseline'u do punktu danych wejściowych. Ścieżka ta jest zwykle linią prostą w przestrzeni wejściowej łączącą baseline z rzeczywistymi danymi wejściowymi - interpolacja między obrazami (baseline i dane wejściowe) za pomocą kroków α od 0 do 1.\n",
        "\n",
        "3. Oblicz gradienty danych wyjściowych modelu w odniesieniu do cech wejściowych w wielu punktach wzdłuż skonstruowanej ścieżki.\n",
        "\n",
        "4. Integracja tych gradientów wzdłuż ścieżki. Jest to często wykonywane przy użyciu numerycznych metod całkowania.\n",
        "\n",
        "5. Zintegrowane gradienty reprezentują wkład każdej cechy w wynik modelu. Wyższe wartości wskazują, że odpowiednia cecha ma bardziej znaczący wpływ na prognozę.\n",
        "\n",
        "![ComparisonOfModels.png](https://i.postimg.cc/TPMsSt7r/Microsoft-Teams-image.png)\n",
        "*Rysunek 1. Porównanie dokładności klasyfikacji na zbiorze StreetsAndHouses dla poszczególnych modeli wraz z uwzględnieniem ich wielkości.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O10hYTrFZTMp"
      },
      "source": [
        "## 2. Konfiguracja środowiska, zmiennych i elementów pomocniczych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjoqbwi6YN_k"
      },
      "outputs": [],
      "source": [
        "# Do poprawnego działania i szybkiej konfiguracji środowiska sugerujemy pracować na platformie Google Colab\n",
        "\n",
        "!pip install captum\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Ie0FLMsY3YT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torchvision as tv\n",
        "\n",
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr import visualization as viz\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from transformers import ConvNextV2ForImageClassification\n",
        "from transformers.models.convnextv2.modeling_convnextv2 import ConvNextV2Embeddings\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdhI4Q4_ZA8Q"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tT0m5zoFnb9t"
      },
      "outputs": [],
      "source": [
        "# Mapowanie predykowanych labeli przez sieci do odpowiednich nazw ze zbioru treningowego\n",
        "LABELS = {\n",
        "    0: \"apartment\",\n",
        "    1: \"bath\",\n",
        "    2: \"bed\",\n",
        "    3: \"church\",\n",
        "    4: \"commercial\",\n",
        "    5: \"din\",\n",
        "    6: \"garage\",\n",
        "    7: \"house\",\n",
        "    8: \"industrial\",\n",
        "    9: \"kitchen\",\n",
        "    10: \"living\",\n",
        "    11: \"retail\",\n",
        "    12: \"roof\" }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bLlxxq56osfH"
      },
      "outputs": [],
      "source": [
        "# Określenie liczby kanałów w obrazach (3 bo RGB) i liczby predykowanych klas (13)\n",
        "IN_CHANNELS = 3\n",
        "N_CLASSES = 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0gdrxSoVobfw"
      },
      "outputs": [],
      "source": [
        "# Transformacja obrazu do odpowiedniej wielkości, zamiana na tensor i normalizacja (ImageNet-1k defaults)\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ2INYEBnRt0"
      },
      "source": [
        "## 3. Definicja modeli i wgranie wag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHcts_ysoIQi"
      },
      "source": [
        "### 3.1. VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odVdJirmnCZN"
      },
      "outputs": [],
      "source": [
        "vgg16_model = tv.models.vgg16(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = vgg16_model.classifier[6].in_features\n",
        "vgg16_model.classifier[6] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))\n",
        "\n",
        "# !!! TODO: Uzupełnij ścieżkę do checkpointa:\n",
        "vgg16_model.load_state_dict(torch.load(\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z3eUuuFprpk"
      },
      "source": [
        "### 3.2. ResNet-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr7DvOXj1iTS"
      },
      "outputs": [],
      "source": [
        "resnet50_model = tv.models.resnet50(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = resnet50_model.fc.in_features\n",
        "resnet50_model.fc = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))\n",
        "\n",
        "# !!! TODO: Uzupełnij ścieżkę do checkpointa:\n",
        "resnet50_model.load_state_dict(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI4639UAp2HB"
      },
      "source": [
        "### 3.3. Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHqzDMRk1pKq"
      },
      "outputs": [],
      "source": [
        "swin_model = tv.models.swin_b(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = swin_model.head.in_features\n",
        "swin_model.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))\n",
        "\n",
        "# !!! TODO: Uzupełnij ścieżkę do checkpointa:\n",
        "resnet50_model.load_state_dict(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ8a78h-p7TU"
      },
      "source": [
        "### 3.4. ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqmICKo5qEPy"
      },
      "outputs": [],
      "source": [
        "convnext_model = tv.models.convnext_base(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = convnext_model.classifier[2].in_features\n",
        "# !!! TODO: Dodaj głowę klasyfikacyjną dla modelu ConvNeXt w analogiczny sposób jak przy poprzednich.\n",
        "# Wskazówka: Wyprintuj sieć (w osobnej komórce: convnext_model) -> znajdź w którym miejscu znajduje się warstwa\n",
        "# fully-connected w bloku classifier -> zastąp ją warstwą klasyfikacyjną taka jaka jest zaimplementowana dla ConvNeXt V2.\n",
        "\n",
        "# !!! TODO: Uzupełnij ścieżkę do checkpointa:\n",
        "convnext_model.load_state_dict(torch.load(\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK5C8abXp_cN"
      },
      "source": [
        "### 3.5. ConvNeXt V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlK8efFp1NH"
      },
      "outputs": [],
      "source": [
        "convnextv2_model = ConvNextV2ForImageClassification.from_pretrained(\"facebook/convnextv2-base-1k-224\")\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = convnextv2_model.classifier.in_features\n",
        "convnextv2_model.classifier = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.4),\n",
        "    nn.Linear(256, N_CLASSES), nn.Softmax(dim=1))\n",
        "\n",
        "# !!! TODO: Uzupełnij ścieżkę do checkpointa:\n",
        "convnextv2_model.load_state_dict(torch.load(\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8akEQKMptJ4w"
      },
      "source": [
        "## 4. Klasyfikacja zdjęć ulic i mieszkań"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_5jjL2AtTiq"
      },
      "outputs": [],
      "source": [
        "# TODO: Znajdź 2 zdjęcia (jedno z Google Maps i jedno z Airbnb) przedstawiające budynek/pokój \n",
        "# należący do którejś z klas ze słownika LABELS.\n",
        "# Następnie dokonaj klasyfikacji tego obrazu przy użyciu każdej z sieci -> Porównaj wyniki\n",
        "# Wskazówka: Najlepiej użyć pochodzących z obszaru USA z racji, że modele trenowane były właśnie na \n",
        "# zdjęciach pochodzących z tego kraju.\n",
        "\n",
        "image1 = Image.open(\"PATH-TO-IMAGE-FROM-GOOGLE-MAPS\")\n",
        "image2 = Image.open(\"PATH-TO-IMAGE-FROM-AIRBNB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgOG5l63v6Ao"
      },
      "outputs": [],
      "source": [
        "image1 = img_transform(image1)\n",
        "image2 = img_transform(image2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfL3_f-4wzmX"
      },
      "outputs": [],
      "source": [
        "image1 = image1.to(device)\n",
        "image2 = image2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3N41JW0wm8X"
      },
      "source": [
        "### 4.1. VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru6N3Y0uxwgs"
      },
      "outputs": [],
      "source": [
        "vgg16_model = vgg16_model.to(device)\n",
        "vgg16_model.eval()\n",
        "\n",
        "vgg16_img1_out = vgg16_model(image1)\n",
        "_, img1_class_vgg16 = torch.max(vgg16_img1_out[0], 1)\n",
        "\n",
        "vgg16_img2_out = vgg16_model(image2)\n",
        "_, img2_class_vgg16 = torch.max(vgg16_img2_out[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFGz4mE6xgKF"
      },
      "source": [
        "### 4.2. ResNet-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uRRpFiJ1zIf"
      },
      "outputs": [],
      "source": [
        "resnet50_model = resnet50_model.to(device)\n",
        "resnet50_model.eval()\n",
        "\n",
        "resnet50_img1_out = vgg16_model(image1)\n",
        "_, img1_class_resnet50 = torch.max(resnet50_img1_out[0], 1)\n",
        "\n",
        "resnet50_img2_out = vgg16_model(image2)\n",
        "_, img2_class_resnet50 = torch.max(resnet50_img2_out[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_elJ0kqxf4b"
      },
      "source": [
        "### 4.3. Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAfLWRqJ12qR"
      },
      "outputs": [],
      "source": [
        "swin_model = swin_model.to(device)\n",
        "swin_model.eval()\n",
        "\n",
        "swin_img1_out = vgg16_model(image1)\n",
        "_, img1_class_swin = torch.max(swin_img1_out[0], 1)\n",
        "\n",
        "swin_img2_out = vgg16_model(image2)\n",
        "_, img2_class_swin = torch.max(swin_img2_out[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbGAQO66xfj9"
      },
      "source": [
        "### 4.4. ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zndSv4pwDBi"
      },
      "outputs": [],
      "source": [
        "convnext_model = convnext_model.to(device)\n",
        "convnext_model.eval()\n",
        "\n",
        "convnext_img1_out = convnext_model(image1)\n",
        "_, img1_class_convnext = torch.max(convnext_img1_out[0], 1)\n",
        "\n",
        "convnext_img2_out = convnext_model(image2)\n",
        "_, img2_class_convnext = torch.max(convnext_img2_out[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI9Q_XJVxp9L"
      },
      "source": [
        "### 4.5. ConvNeXt V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZn4Z6BkxraV"
      },
      "outputs": [],
      "source": [
        "convnextv2_model = convnextv2_model.to(device)\n",
        "convnextv2_model.eval()\n",
        "\n",
        "convnextv2_img1_out = convnextv2_model(image1)\n",
        "_, img1_class_convnextv2 = torch.max(convnextv2_img1_out[0], 1)\n",
        "\n",
        "convnextv2_img2_out = convnextv2_model(image2)\n",
        "_, img2_class_convnextv2 = torch.max(convnextv2_img2_out[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWQn8o0Ux2tf"
      },
      "source": [
        "## 5. Interpretowalność predykcji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tRE24l6Fx6Ez"
      },
      "outputs": [],
      "source": [
        "# TODO: Dokonaj analizy interpretowalności dla poszczególnych modeli i wybranych wcześniej obrazów."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18H-_1Dkzb_9"
      },
      "source": [
        "### 5.1. VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucyF2SIW1Oqo"
      },
      "outputs": [],
      "source": [
        "# Objekt IntegratedGradient i atrybuty\n",
        "integrated_gradients_vgg16 = IntegratedGradients(vgg16_model)\n",
        "\n",
        "attributions_ig_vgg16_img1 = integrated_gradients_vgg16.attribute(image1, target=img1_class_vgg16, n_steps=30)\n",
        "attributions_ig_vgg16_img2 = integrated_gradients_vgg16.attribute(image2, target=img2_class_vgg16, n_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3uLR1441SOw"
      },
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_vgg16_img1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwzM9Vw-1dER"
      },
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_vgg16_img2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WADAvvrh19XY"
      },
      "source": [
        "### 5.2. ResNet-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Objekt IntegratedGradient i atrybuty\n",
        "integrated_gradients_resnet = IntegratedGradients(resnet50_model)\n",
        "\n",
        "attributions_ig_resnet_img1 = integrated_gradients_resnet.attribute(image1, target=img1_class_resnet50, n_steps=30)\n",
        "attributions_ig_resnet_img2 = integrated_gradients_resnet.attribute(image2, target=img2_class_resnet50, n_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_resnet_img1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_resnet_img2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjqUIqub19Q5"
      },
      "source": [
        "### 5.3. Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Objekt IntegratedGradient i atrybuty\n",
        "integrated_gradients_swin = IntegratedGradients(swin_model)\n",
        "\n",
        "attributions_ig_swin_img1 = integrated_gradients_swin.attribute(image1, target=img1_class_swin, n_steps=30)\n",
        "attributions_ig_swin_img2 = integrated_gradients_swin.attribute(image2, target=img2_class_swin, n_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_swin_img1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_swin_img2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReUy0NeH0h3-"
      },
      "source": [
        "### 5.4. ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI3X6_Z3zPn7"
      },
      "outputs": [],
      "source": [
        "# Objekt IntegratedGradient i atrybuty\n",
        "integrated_gradients_convnext = IntegratedGradients(convnext_model)\n",
        "\n",
        "attributions_ig_convnext_img1 = integrated_gradients_convnext.attribute(image1, target=img1_class_convnext, n_steps=30)\n",
        "attributions_ig_convnext_img2 = integrated_gradients_convnext.attribute(image2, target=img2_class_convnext, n_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVsVjMub0TiA"
      },
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_convnext_img1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5TosZmA0bxP"
      },
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_convnext_img2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3BnW7BV0khm"
      },
      "source": [
        "### 5.5. ConvNeXt V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41zPNvFk0nrz"
      },
      "outputs": [],
      "source": [
        "# Objekt IntegratedGradient i atrybuty\n",
        "integrated_gradients_convnextv2 = IntegratedGradients(convnextv2_model)\n",
        "\n",
        "attributions_ig_convnextv2_img1 = integrated_gradients_convnextv2.attribute(image1, target=img1_class_convnextv2, n_steps=30)\n",
        "attributions_ig_convnextv2_img2 = integrated_gradients_convnextv2.attribute(image2, target=img2_class_convnextv2, n_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dytOqLXY1EkS"
      },
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_convnextv2_img1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image1.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vLyXtQw1Kj5"
      },
      "outputs": [],
      "source": [
        "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_convnextv2_img2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             np.transpose(image2.squeeze(dim=0).cpu().detach().numpy(), (1,2,0)),\n",
        "                             methods=[\"original_image\", \"heat_map\"],\n",
        "                             signs=['all', 'positive'],\n",
        "                             cmap=\"viridis\",\n",
        "                             show_colorbar=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
