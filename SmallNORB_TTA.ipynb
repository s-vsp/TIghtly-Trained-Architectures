{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK74_AGm9AQy"
      },
      "source": [
        "# SmallNORB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELoZh7xwXk3H"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saDmHjP_XzSP",
        "outputId": "58cc84cb-f8a0-4df6-b719-10b9fcbdcee6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\daria\\Desktop\\multimodal-storytelling\\.venv0\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from datasets import load_dataset\n",
        "from transformers import ConvNextV2ForImageClassification\n",
        "from transformers.models.convnextv2.modeling_convnextv2 import ConvNextV2Embeddings\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIrW-pXG3-Oa",
        "outputId": "54c433d1-fddc-4fa0-ae8b-70f7ee54d329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 11 11:18:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 528.92       Driver Version: 528.92       CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   39C    P8    13W /  50W |    662MiB /  8192MiB |      9%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A       428    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A      2756    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A      4728    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      7268    C+G   ...eripheral Manager\\DPM.exe    N/A      |\n",
            "|    0   N/A  N/A      7668    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A      9032    C+G   ...151.44\\msedgewebview2.exe    N/A      |\n",
            "|    0   N/A  N/A     11680    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A     12916    C+G   ...a0\\XboxGameBarSpotify.exe    N/A      |\n",
            "|    0   N/A  N/A     13616    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     16936    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A     16996    C+G   ...151.44\\msedgewebview2.exe    N/A      |\n",
            "|    0   N/A  N/A     18024    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     18808    C+G   ...oft OneDrive\\OneDrive.exe    N/A      |\n",
            "|    0   N/A  N/A     21408    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     21700    C+G   ...2gh52qy24etm\\Nahimic3.exe    N/A      |\n",
            "|    0   N/A  N/A     22076    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
            "|    0   N/A  N/A     23812    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRMFEAwCgwMx",
        "outputId": "96c49df2-8556-4459-bb61-1d5c4ea2c997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7YonrHpYBQU"
      },
      "outputs": [],
      "source": [
        "smallnorb = load_dataset(\"Ramos-Ramos/smallnorb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIX0_kZKYD_d"
      },
      "outputs": [],
      "source": [
        "class SmallNORBDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, hf_dataset, subset=\"train\", transform=None):\n",
        "    self.hf_dataset = hf_dataset\n",
        "    self.subset = subset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.hf_dataset[self.subset])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.hf_dataset[self.subset][idx]\n",
        "    image = sample[\"image_lt\"]\n",
        "    label = sample[\"category\"]\n",
        "\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCh5SuzjdNWn"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rcaE-4if8S_"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(224,224), scale=(0.8, 1.0)),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229])\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229])\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Yy-i_qWeiSk"
      },
      "outputs": [],
      "source": [
        "train_ds = SmallNORBDataset(smallnorb, \"train\", transform=transforms_train)\n",
        "val_ds = SmallNORBDataset(smallnorb, \"train\", transform=transforms_val)\n",
        "\n",
        "train_size = int(0.8 * len(train_ds))\n",
        "val_size = len(train_ds) - train_size\n",
        "\n",
        "train_ds, _ = torch.utils.data.random_split(train_ds, [train_size, val_size])\n",
        "_, val_ds = torch.utils.data.random_split(val_ds, [train_size, val_size])\n",
        "\n",
        "test_ds = SmallNORBDataset(smallnorb, \"test\", transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el1AB7KlgSTD"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 1\n",
        "N_CLASSES = 5\n",
        "\n",
        "vgg16_model = tv.models.vgg16(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z49kALI7L3-A"
      },
      "outputs": [],
      "source": [
        "for param in vgg16_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doYJYuJcLwXD"
      },
      "outputs": [],
      "source": [
        "# 1-channel inputs\n",
        "vgg16_model.features[0] = nn.Conv2d(IN_CHANNELS, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = vgg16_model.classifier[6].in_features\n",
        "vgg16_model.classifier[6] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMKcEC0CmYAA"
      },
      "outputs": [],
      "source": [
        "vgg16_model = vgg16_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nNEJk5zL8wt",
        "outputId": "ae59331f-e021-4442-ea69-29a010ecb6d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135,309,509 total parameters.\n",
            "1,050,757 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in vgg16_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in vgg16_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzcNgOXbnfI8"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 35\n",
        "LEARNING_RATE = 1e-2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwhtckiYpIpc"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfts5f9upscs"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(vgg16_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer=optimizer, mode=\"max\", factor=0.1, patience=11, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18Hd-yk0qBiY"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  vgg16_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = vgg16_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  vgg16_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = vgg16_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step(val_loss)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(vgg16_model.state_dict(), \"vgg16-smallnorb.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWOVyxO6AwPJ",
        "outputId": "9500fe91-dee3-4325-892f-31ef4278e784"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████████████████████████████████████████████| 1519/1519 [01:37<00:00, 15.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy = 91.2305%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "vgg16_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = vgg16_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VChpNC18JsT"
      },
      "source": [
        "## ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reQoTIjUBRlu"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEVYtdrzBRHU"
      },
      "outputs": [],
      "source": [
        "train_ds = SmallNORBDataset(smallnorb, \"train\", transform=transforms_train)\n",
        "val_ds = SmallNORBDataset(smallnorb, \"train\", transform=transforms_val)\n",
        "\n",
        "train_size = int(0.8 * len(train_ds))\n",
        "val_size = len(train_ds) - train_size\n",
        "\n",
        "train_ds, _ = torch.utils.data.random_split(train_ds, [train_size, val_size])\n",
        "_, val_ds = torch.utils.data.random_split(val_ds, [train_size, val_size])\n",
        "\n",
        "test_ds = SmallNORBDataset(smallnorb, \"test\", transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2mChAWA9WG1"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 1\n",
        "N_CLASSES = 5\n",
        "\n",
        "convnext_model = tv.models.convnext_base(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bhYWUeG-sCM"
      },
      "outputs": [],
      "source": [
        "for param in convnext_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t94eFe86_Foj"
      },
      "outputs": [],
      "source": [
        "# 1-channel inputs\n",
        "convnext_model.features[0][0] = nn.Conv2d(IN_CHANNELS, 128, kernel_size=(4,4), stride=(4,4))\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = convnext_model.classifier[2].in_features\n",
        "convnext_model.classifier[2] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.4),\n",
        "    nn.Linear(256, N_CLASSES), nn.Softmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chpdax503-Oh"
      },
      "outputs": [],
      "source": [
        "convnext_model = convnext_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZVnRzAjIR0b",
        "outputId": "9bbeb2e4-72ee-4219-9099-ee21410cc821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87,826,053 total parameters.\n",
            "265,861 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in convnext_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in convnext_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq2s9XX0ALKz"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 5e-5\n",
        "WEIGHT_DECAY = 1e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLXn7E1KsCJZ"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW26rX_6uEH6"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(convnext_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK4UPDbtzLkP"
      },
      "outputs": [],
      "source": [
        "# Additional ConvNeXt augmentations\n",
        "cutmix = v2.CutMix(num_classes=N_CLASSES)\n",
        "mixup = v2.MixUp(num_classes=N_CLASSES)\n",
        "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek51Wxjlw8cV"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  convnext_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs, labels = cutmix_or_mixup(inputs, labels)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = convnext_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  convnext_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnext_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(convnext_model.state_dict(), \"convnext-smallnorb.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o7BDh_G3-Oj",
        "outputId": "58a7b80e-40ae-47a3-c3cc-d51f348903ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████████████████████████████████████████████| 1519/1519 [02:43<00:00,  9.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy = 92.4979%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "convnext_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnext_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBYonKd93-Ok"
      },
      "source": [
        "## ConvNeXt V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODmK-9UT3-Ok"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485], [0.229]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrBAO5IG3-Ok"
      },
      "outputs": [],
      "source": [
        "train_ds = SmallNORBDataset(smallnorb, \"train\", transform=transforms_train)\n",
        "val_ds = SmallNORBDataset(smallnorb, \"train\", transform=transforms_val)\n",
        "\n",
        "train_size = int(0.8 * len(train_ds))\n",
        "val_size = len(train_ds) - train_size\n",
        "\n",
        "train_ds, _ = torch.utils.data.random_split(train_ds, [train_size, val_size])\n",
        "_, val_ds = torch.utils.data.random_split(val_ds, [train_size, val_size])\n",
        "\n",
        "test_ds = SmallNORBDataset(smallnorb, \"test\", transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6q1AINZz3-O2"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 1\n",
        "N_CLASSES = 5\n",
        "\n",
        "convnextv2_model = ConvNextV2ForImageClassification.from_pretrained(\"facebook/convnextv2-base-1k-224\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waPyQQYP3-O2"
      },
      "outputs": [],
      "source": [
        "for param in convnextv2_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT8ZgayX3-O2"
      },
      "outputs": [],
      "source": [
        "# 1-channel inputs\n",
        "convnextv2_model.convnextv2.embeddings.patch_embeddings = nn.Conv2d(IN_CHANNELS, 128, kernel_size=(4,4), stride=(4,4))\n",
        "convnextv2_model.convnextv2.embeddings.num_channels = IN_CHANNELS\n",
        "\n",
        "# Add on classifier\n",
        "n_inputs = convnextv2_model.classifier.in_features\n",
        "convnextv2_model.classifier = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.4),\n",
        "    nn.Linear(256, N_CLASSES), nn.Softmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSnacLDY3-O2"
      },
      "outputs": [],
      "source": [
        "convnextv2_model = convnextv2_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh34NSi43-O3",
        "outputId": "d2a43ab2-be03-4f0f-b00a-97892634a2bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87,952,389 total parameters.\n",
            "265,861 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in convnextv2_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in convnextv2_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t1xmLj43-O3"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 6.25e-3\n",
        "WEIGHT_DECAY = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBF_6iRK3-O3"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70UYGLYx3-O3"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(convnextv2_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_aCkGCO3-O3"
      },
      "outputs": [],
      "source": [
        "# Additional ConvNeXt augmentations\n",
        "cutmix = v2.CutMix(num_classes=N_CLASSES)\n",
        "mixup = v2.MixUp(alpha=0.8, num_classes=N_CLASSES)\n",
        "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbXV2OTi3-O4"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  convnextv2_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs, labels = cutmix_or_mixup(inputs, labels)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = convnextv2_model(inputs)\n",
        "    loss = criterion(outputs[0], labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  convnextv2_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnextv2_model(inputs)\n",
        "      loss = criterion(outputs[0], labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs[0], 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(convnextv2_model.state_dict(), \"convnextv2-smallnorb.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCftAdfv3-O4",
        "outputId": "4bd281c0-abdf-442e-edc5-3e3f21643fee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████████████████████████████████████████████| 1519/1519 [03:43<00:00,  6.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy = 92.5267%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "convnextv2_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnextv2_model(inputs)\n",
        "      loss = criterion(outputs[0], labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs[0], 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improved ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Swin Transformer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
