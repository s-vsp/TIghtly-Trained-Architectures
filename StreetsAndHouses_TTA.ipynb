{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSgrytK-aN5X",
        "outputId": "2c531217-9ba0-493d-e57a-7c2fa59d9460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.5)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "au7w2x8pa8aP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import v2\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "from transformers import ConvNextV2ForImageClassification\n",
        "from transformers.models.convnextv2.modeling_convnextv2 import ConvNextV2Embeddings\n",
        "\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKiwOUwPhN45",
        "outputId": "93af8d40-2d91-4b6c-9230-05b7e83f57b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njsA9yP9a6al",
        "outputId": "db66da73-5d78-49da-d7e8-62c011fd91d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asSt0LgKbHmL",
        "outputId": "ce014eb0-a3c2-4fd9-ebbf-04f725ed833d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace kaggle_room_street_data/house_data/bath_100.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q /content/drive/MyDrive/UM_Project/datasets/streetshouses.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "giCb02-qZg0b"
      },
      "outputs": [],
      "source": [
        "class StreetAndHousesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transform=None):\n",
        "        self.files = files\n",
        "        self.transform = transform\n",
        "        self.file_paths = list()\n",
        "        self.labels = list()\n",
        "\n",
        "        # Real labels\n",
        "        labels_keys = [\"apartment\", \"bath\", \"bed\", \"church\", \\\n",
        "                  \"commercial\", \"din\", \"garage\", \"house\", \"industrial\", \"kitchen\", \"living\", \"retail\", \"roof\"]\n",
        "\n",
        "        LABELS = {key: val for key, val in zip(labels_keys, list(range(len(labels_keys))))}\n",
        "\n",
        "        for file_path in files:\n",
        "          if \"BdIdx\" not in file_path:\n",
        "            label = os.path.basename(file_path).split(\"_\")[0]\n",
        "          else:\n",
        "            label = os.path.basename(file_path).split(\"_\")[-1][:-4]\n",
        "\n",
        "          # Additional cleaning of labels is required\n",
        "          if label == \"apartments\":\n",
        "            label = \"apartment\"\n",
        "          if label == \"garages\":\n",
        "            label = \"garage\"\n",
        "          if label == \"office\":\n",
        "            label = \"officebuilding\"\n",
        "\n",
        "          if label in LABELS.keys():\n",
        "            self.file_paths.append(file_path)\n",
        "            self.labels.append(LABELS[label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSQ8H34i1iS"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awt4N46ii5iW"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(224,224), scale=(0.8, 1.0)),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtupuxizUi3f"
      },
      "outputs": [],
      "source": [
        "# Create paths for val, train and test (with shuffling the data)\n",
        "dirs = [os.path.join(\"./streetshouses/kaggle_room_street_data/\", path) for path in os.listdir(\"./streetshouses/kaggle_room_street_data/\")]\n",
        "\n",
        "all_files = []\n",
        "\n",
        "for dir_path in dirs:\n",
        "    files = [os.path.join(dir_path, file) for file in os.listdir(dir_path)]\n",
        "    all_files.extend(files)\n",
        "\n",
        "train_paths, test_paths = train_test_split(all_files, train_size=0.9, random_state=42, shuffle=True)\n",
        "train_paths, val_paths = train_test_split(train_paths, train_size=0.78, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAzPE9jDjMzf"
      },
      "outputs": [],
      "source": [
        "train_ds = StreetAndHousesDataset(files=train_paths, transform=transforms_train)\n",
        "val_ds = StreetAndHousesDataset(files=val_paths, transform=transforms_val)\n",
        "test_ds = StreetAndHousesDataset(files=test_paths, transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqT0J1vlcroe"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 3\n",
        "N_CLASSES = 13\n",
        "\n",
        "vgg16_model = tv.models.vgg16(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKmlZLLFdLzW"
      },
      "outputs": [],
      "source": [
        "for param in vgg16_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO_hm6-lhFWa"
      },
      "outputs": [],
      "source": [
        "# Add on classifier\n",
        "n_inputs = vgg16_model.classifier[6].in_features\n",
        "vgg16_model.classifier[6] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLPnawYhH2V"
      },
      "outputs": [],
      "source": [
        "vgg16_model = vgg16_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUfUyaDghKNk",
        "outputId": "91aa4220-9ddb-462a-fd1c-8ecbae8d06c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "135,312,717 total parameters.\n",
            "1,052,173 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in vgg16_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in vgg16_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi-ARrtwha6I"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 35\n",
        "LEARNING_RATE = 1e-2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxUlVAHghksI"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGnWKj2OhnIX"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(vgg16_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer=optimizer, mode=\"max\", factor=0.1, patience=6, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX0lYa1bhqyI"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  vgg16_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = vgg16_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  vgg16_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = vgg16_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step(val_loss)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(vgg16_model.state_dict(), \"/content/drive/MyDrive/checkpoints/vgg16-streetsandhouses.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAhD5vGlrqoq",
        "outputId": "58a52be9-aea6-47b1-92a1-0c84fe2601c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|████████████████████████████████████████████████████| 141/141 [00:38<00:00,  3.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy = 51.6644%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "vgg16_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = vgg16_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_fCozvFWKLv"
      },
      "source": [
        "## ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ6pr0_6VUFb"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5dDy9zfWb8x"
      },
      "outputs": [],
      "source": [
        "# Create paths for val, train and test (with shuffling the data)\n",
        "dirs = [os.path.join(\"./streetshouses/kaggle_room_street_data/\", path) for path in os.listdir(\"./streetshouses/kaggle_room_street_data/\")]\n",
        "\n",
        "all_files = []\n",
        "\n",
        "for dir_path in dirs:\n",
        "    files = [os.path.join(dir_path, file) for file in os.listdir(dir_path)]\n",
        "    all_files.extend(files)\n",
        "\n",
        "train_paths, test_paths = train_test_split(all_files, train_size=0.9, random_state=42, shuffle=True)\n",
        "train_paths, val_paths = train_test_split(train_paths, train_size=0.78, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl7SFLYHXglL"
      },
      "outputs": [],
      "source": [
        "train_ds = StreetAndHousesDataset(files=train_paths, transform=transforms_train)\n",
        "val_ds = StreetAndHousesDataset(files=val_paths, transform=transforms_val)\n",
        "test_ds = StreetAndHousesDataset(files=test_paths, transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFYLdKbuXjty"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 3\n",
        "N_CLASSES = 13\n",
        "\n",
        "convnext_model = tv.models.convnext_base(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fy-7uuT-YugV"
      },
      "outputs": [],
      "source": [
        "for param in convnext_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfxnFOe4Yyz8"
      },
      "outputs": [],
      "source": [
        "# Add on classifier\n",
        "n_inputs = convnext_model.classifier[2].in_features\n",
        "convnext_model.classifier[2] = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.4),\n",
        "    nn.Linear(256, N_CLASSES), nn.Softmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOgAipSoY5xi"
      },
      "outputs": [],
      "source": [
        "convnext_model = convnext_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf9obso9Y7oC",
        "outputId": "6813339f-7d87-467f-b0ae-9ffbc3d1b3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87,832,205 total parameters.\n",
            "265,741 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in convnext_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in convnext_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4U64dF_Y9bK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 5e-5\n",
        "WEIGHT_DECAY = 1e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0Zd3O9XZG34"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV3nt6_AZIiJ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(convnext_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwW8IZniZPMB"
      },
      "outputs": [],
      "source": [
        "# Additional ConvNeXt augmentations\n",
        "cutmix = v2.CutMix(num_classes=N_CLASSES)\n",
        "mixup = v2.MixUp(num_classes=N_CLASSES)\n",
        "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_WI_nnaZR0H"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  convnext_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = convnext_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  convnext_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnext_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(convnext_model.state_dict(), \"./convnext-streetsandhouses.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtelG4yJa3NN",
        "outputId": "ef06851d-aff6-4430-8f0a-4311ddf01b9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|████████████████████████████████████████████████████| 141/141 [00:17<00:00,  8.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy = 69.4629%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "convnext_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnext_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrHJnDnTJ9mb"
      },
      "source": [
        "## ConvNeXt V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skz8BYWDJ9mb"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwNL3EEKJ9mc"
      },
      "outputs": [],
      "source": [
        "# Create paths for val, train and test (with shuffling the data)\n",
        "dirs = [os.path.join(\"./streetshouses/kaggle_room_street_data/\", path) for path in os.listdir(\"./streetshouses/kaggle_room_street_data/\")]\n",
        "\n",
        "all_files = []\n",
        "\n",
        "for dir_path in dirs:\n",
        "    files = [os.path.join(dir_path, file) for file in os.listdir(dir_path)]\n",
        "    all_files.extend(files)\n",
        "\n",
        "train_paths, test_paths = train_test_split(all_files, train_size=0.9, random_state=42, shuffle=True)\n",
        "train_paths, val_paths = train_test_split(train_paths, train_size=0.78, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X92omWPsJ9mv"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 3\n",
        "N_CLASSES = 13\n",
        "\n",
        "convnextv2_model = ConvNextV2ForImageClassification.from_pretrained(\"facebook/convnextv2-base-1k-224\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBVW0NkLJ9mw"
      },
      "outputs": [],
      "source": [
        "for param in convnextv2_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAOdqQ3XJ9mw"
      },
      "outputs": [],
      "source": [
        "# Add on classifier\n",
        "n_inputs = convnextv2_model.classifier.in_features\n",
        "convnextv2_model.classifier = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.4),\n",
        "    nn.Linear(256, N_CLASSES), nn.Softmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s6M25fzJ9mw"
      },
      "outputs": [],
      "source": [
        "convnextv2_model = convnextv2_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGjpdSzvJ9mw",
        "outputId": "3d354535-bdd4-421a-e4e7-df7b88bbf46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87,958,541 total parameters.\n",
            "265,741 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in convnextv2_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in convnextv2_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmlXb02KJ9mx"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 6.25e-4\n",
        "WEIGHT_DECAY = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APaos1fDJ9mx"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8p-7zuCJ9mx"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(convnextv2_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z--q80yrJ9mx"
      },
      "outputs": [],
      "source": [
        "# Additional ConvNeXt augmentations\n",
        "cutmix = v2.CutMix(num_classes=N_CLASSES)\n",
        "mixup = v2.MixUp(alpha=0.8, num_classes=N_CLASSES)\n",
        "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6loMaeoLJ9mz"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS - 17):\n",
        "  # Train\n",
        "  convnextv2_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs, labels = cutmix_or_mixup(inputs, labels)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = convnextv2_model(inputs)\n",
        "    loss = criterion(outputs[0], labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  convnextv2_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnextv2_model(inputs)\n",
        "      loss = criterion(outputs[0], labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs[0], 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(convnextv2_model.state_dict(), \"convnextv2-streetsandhouses.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txTQm0TvJ9mz",
        "outputId": "6da14da4-aa7c-4708-83d8-d3e410313cee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing:   0%|                                                              | 0/141 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|████████████████████████████████████████████████████| 141/141 [00:51<00:00,  2.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy = 70.2175%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "convnextv2_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = convnextv2_model(inputs)\n",
        "      loss = criterion(outputs[0], labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs[0], 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALHgwzPuJ9m0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ1AUnkbPdxL"
      },
      "source": [
        "## Improved ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXZI1B_JPmUr"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(224,224), scale=(0.8, 1.0)),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al7cQLe-PsRS"
      },
      "outputs": [],
      "source": [
        "# Create paths for val, train and test (with shuffling the data)\n",
        "dirs = [os.path.join(\"./kaggle_room_street_data/\", path) for path in os.listdir(\"./kaggle_room_street_data/\")]\n",
        "\n",
        "all_files = []\n",
        "\n",
        "for dir_path in dirs:\n",
        "    files = [os.path.join(dir_path, file) for file in os.listdir(dir_path)]\n",
        "    all_files.extend(files)\n",
        "\n",
        "train_paths, test_paths = train_test_split(all_files, train_size=0.9, random_state=42, shuffle=True)\n",
        "train_paths, val_paths = train_test_split(train_paths, train_size=0.78, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t1GStfgPtur"
      },
      "outputs": [],
      "source": [
        "train_ds = StreetAndHousesDataset(files=train_paths, transform=transforms_train)\n",
        "val_ds = StreetAndHousesDataset(files=val_paths, transform=transforms_val)\n",
        "test_ds = StreetAndHousesDataset(files=test_paths, transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvyWrcsAPwKu"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 3\n",
        "N_CLASSES = 13\n",
        "\n",
        "resnet50_model = tv.models.resnet50(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZuMAXoIPzAQ"
      },
      "outputs": [],
      "source": [
        "for param in resnet50_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMQw1Ew7P0t6"
      },
      "outputs": [],
      "source": [
        "# Add on classifier\n",
        "n_inputs = resnet50_model.fc.in_features\n",
        "resnet50_model.fc = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq1J5chMQAim"
      },
      "outputs": [],
      "source": [
        "resnet50_model = resnet50_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wyq6QDGQA3y",
        "outputId": "735bb17d-f582-4b4e-e83f-0cac9353fa43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24,035,917 total parameters.\n",
            "527,885 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in resnet50_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in resnet50_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b55HHvUTQBFa"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 5e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5DTbWvfQBVh"
      },
      "outputs": [],
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtPG3v4xQDe8"
      },
      "outputs": [],
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(resnet50_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer=optimizer, T_max=35\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDzQBRjZQEWH"
      },
      "outputs": [],
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  resnet50_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = resnet50_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  resnet50_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = resnet50_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step(val_loss)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(resnet50_model.state_dict(), \"/content/drive/MyDrive/UM_Project/checkpoints/resnet50-streetsandhouses.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH0f6xCfQFvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d868d7-3d39-4960-f93c-bff7e1228b16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "checkpoint = torch.load(\"drive/MyDrive/UM_Project/checkpoints/resnet50-streetsandhouses.pt\")\n",
        "resnet50_model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIokUXSkQI6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de694f1c-cf00-47fc-a2d9-830f0cd1eb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set accuracy = 49.5595%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "resnet50_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = resnet50_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyZs6YklPh8_"
      },
      "source": [
        "## Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "itZGy4N6Pmme"
      },
      "outputs": [],
      "source": [
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "transforms_val = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oSdao_W0QdqI"
      },
      "outputs": [],
      "source": [
        "# Create paths for val, train and test (with shuffling the data)\n",
        "dirs = [os.path.join(\"./kaggle_room_street_data/\", path) for path in os.listdir(\"./kaggle_room_street_data/\")]\n",
        "\n",
        "all_files = []\n",
        "\n",
        "for dir_path in dirs:\n",
        "    files = [os.path.join(dir_path, file) for file in os.listdir(dir_path)]\n",
        "    all_files.extend(files)\n",
        "\n",
        "train_paths, test_paths = train_test_split(all_files, train_size=0.9, random_state=42, shuffle=True)\n",
        "train_paths, val_paths = train_test_split(train_paths, train_size=0.78, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = StreetAndHousesDataset(files=train_paths, transform=transforms_train)\n",
        "val_ds = StreetAndHousesDataset(files=val_paths, transform=transforms_val)\n",
        "test_ds = StreetAndHousesDataset(files=test_paths, transform=transforms_test)"
      ],
      "metadata": {
        "id": "da3wd7wLUMCv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "IF0Pb5fRQebT"
      },
      "outputs": [],
      "source": [
        "IN_CHANNELS = 3\n",
        "N_CLASSES = 13\n",
        "\n",
        "swin_model = tv.models.swin_b(weights=\"IMAGENET1K_V1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uAIYfi3KQeZK"
      },
      "outputs": [],
      "source": [
        "for param in swin_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "lu_M933lQeWT"
      },
      "outputs": [],
      "source": [
        "# Add on classifier\n",
        "n_inputs = swin_model.head.in_features\n",
        "swin_model.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 256), nn.GELU(), nn.Dropout(0.6),\n",
        "    nn.Linear(256, N_CLASSES), nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "swin_model = swin_model.to(device)"
      ],
      "metadata": {
        "id": "9YO3KUv_TEk4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in swin_model.parameters())\n",
        "print(f\"{total_params:,} total parameters.\")\n",
        "total_trainable_params = sum(\n",
        "    p.numel() for p in swin_model.parameters() if p.requires_grad)\n",
        "print(f\"{total_trainable_params:,} trainable parameters.\")"
      ],
      "metadata": {
        "id": "CpwCd6LNTEio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8affd4f6-198b-4ffb-b663-a526be0681a1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87,008,965 total parameters.\n",
            "265,741 trainable parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "LEARNING_RATE = 5e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-2"
      ],
      "metadata": {
        "id": "gqMHbsqlTEg5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = torch.utils.data.DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
        "test_dl = torch.utils.data.DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "9NZhGSTFTEe8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(swin_model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer=optimizer, T_max=35\n",
        ")"
      ],
      "metadata": {
        "id": "H4BA1VJPTEc8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = 1e7\n",
        "train_losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Train\n",
        "  swin_model.train()\n",
        "  train_loss = 0.0\n",
        "  for batch in tqdm(train_dl, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", ncols=100):\n",
        "    inputs, labels = batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = swin_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  # Validate\n",
        "  swin_model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_dl, desc=\"Validation\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = swin_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "  lr_scheduler.step(val_loss)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(swin_model.state_dict(), \"/content/drive/MyDrive/UM_Project/checkpoints/swin-streetshouses.pt\")\n",
        "\n",
        "  train_losses.append(train_loss / len(train_dl))\n",
        "  val_losses.append(val_loss / len(val_dl))\n",
        "\n",
        "  accuracy = 100 * correct_predictions / total_predictions\n",
        "  print(f\"Epoch {epoch + 1}/{EPOCHS}, Train Loss: {train_loss / len(train_dl):.4f}, Validation Loss: {val_loss / len(val_dl):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "zStN5jPITEbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO8i43YoQeUQ",
        "outputId": "5fdf394d-0b2f-410a-c908-c14d01dea901"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/UM_Project/checkpoints/swin-streetshouses.pt\")\n",
        "swin_model.load_state_dict(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8V7ak2xQeSK",
        "outputId": "ef3d8141-583e-4723-feb3-0cab5138b680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|████████████████████████████████████████████████████| 142/142 [00:43<00:00,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set accuracy = 62.6872%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Predict on Test set\n",
        "swin_model.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dl, desc=\"Testing\", ncols=100):\n",
        "      inputs, labels = batch\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = swin_model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total_predictions += labels.size(0)\n",
        "      correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"\\nTest set accuracy = {(100 * correct_predictions / total_predictions):.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nH6zBrKdhJxU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PpSQ8H34i1iS",
        "b_fCozvFWKLv",
        "BrHJnDnTJ9mb",
        "fyZs6YklPh8_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}